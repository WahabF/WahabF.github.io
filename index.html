<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Abdulwahab Felemban</title>
  <meta name="author" content="Abdulwahab Felemban">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.75em%22 font-size=%2280%22>üåç</text></svg>">
</head>

<body>
  <div class="background-grid"></div>
  <div class="background-glow"></div>
  <nav class="top-nav">
    <div class="nav-inner">
      <div class="brand">
        <a href="#top">Abdulwahab Felemban</a>
        <span class="location">KAUST, Kingdom of Saudi Arabia</span>
      </div>
      <div class="nav-links">
        <a href="#top">Home</a>
        <a href="#about">About</a>
        <a href="#research-focus">Research</a>
        <a href="#news">News</a>
        <a href="#publications">Publications</a>
        <a href="#projects">Projects</a>
        <a href="#education">Education</a>
        <a href="#awards">Awards</a>
        <a href="#service">Service</a>
      </div>
    </div>
  </nav>

  <header class="hero">
    <div class="hero__content">
      <p class="eyebrow">Multimodal AI ¬∑ LLMs &amp; VLMs ¬∑ Computer Vision ¬∑ Motion</p>
      <h1>Abdulwahab Felemban</h1>
      <p class="lead">
        Fourth-year PhD candidate specializing in multimodal large language models, vision-language models, generative
        models, temporal modeling, motion understanding and generation, and large-scale data processing.<br>
        <span class="highlight-text">Looking for an exciting research internship!</span><br>
        Available to start right away ‚Äî<br>
        please reach out if you have an opening.
      </p>
      <div class="cta-row">
        <a class="cta ghost" href="CV_Felemban.pdf" target="_blank" rel="noreferrer">Full CV</a>
      </div>
      <!-- <div class="pill-strip">
        <span>LLMs ¬∑ VLMs ¬∑ MLLMs</span>
        <span>Reasoning-centric tuning</span>
        <span>Motion forecasting</span>
        <span>Vision-language driving</span>
        <span>Data engines & evaluation</span>
      </div> -->
      <div class="contact-links">
        <a class="cta ghost" href="mailto:abdulwahab.felemban@kaust.edu.sa">
          <svg class="icon" viewBox="0 0 24 24" aria-hidden="true">
            <path
              d="M3 6.5A2.5 2.5 0 0 1 5.5 4h13A2.5 2.5 0 0 1 21 6.5v11a2.5 2.5 0 0 1-2.5 2.5h-13A2.5 2.5 0 0 1 3 17.5v-11Zm2.3-.5 6.2 4.7c.3.23.7.23 1 0L18.7 6H5.3Z"
              fill="currentColor" />
          </svg>
          abdulwahab.felemban@kaust.edu.sa
        </a>
        <a class="cta ghost" href="https://wa.me/966569144133" target="_blank" rel="noreferrer">
          <img class="icon-img" src="images/whatsapp.png" alt="">
          +966569144133
        </a>
        <a class="cta ghost" href="https://www.linkedin.com/in/abdulwahab-felemban-b61469144/" target="_blank"
          rel="noreferrer">
          <img class="icon-img" src="images/linkedin.webp" alt="">
          LinkedIn
        </a>
        <a class="cta ghost" href="https://github.com/WahabF" target="_blank" rel="noreferrer">
          <img class="icon-img" src="images/github.png" alt="">
          GitHub
        </a>
        <a class="cta ghost" href="https://scholar.google.com/citations?user=1jimpTAAAAAJ&hl=en" target="_blank"
          rel="noreferrer">
          <img class="icon-img" src="images/Google_Scholar_logo.svg.png" alt="">
          Google Scholar
        </a>
        <a class="cta ghost" href="https://twitter.com/Ab_flm" target="_blank" rel="noreferrer">
          <img class="icon-img" src="images/x.webp" alt="">
          Twitter
        </a>
        <a class="cta ghost" href="https://www.instagram.com/aa_fel" target="_blank" rel="noreferrer">
          <img class="icon-img" src="images/instagram.png" alt="">
          Instagram
        </a>
        <a class="cta ghost" href="https://boxd.it/dx4h5" target="_blank" rel="noreferrer">
          <img class="icon-img" src="images/letterboxd.png" alt="">
          Letterboxd
        </a>
      </div>
    </div>
    <div class="hero__visual">
      <div class="profile-card">
        <div class="profile-glow"></div>
        <img src="images/profile.jpg" alt="Abdulwahab Felemban" class="profile-photo">
      </div>
    </div>
  </header>

  <main class="page">
    <section class="section" id="about">
      <div class="section__header">
        <div>
          <p class="eyebrow">About</p>
        </div>
      </div>
      <p class="body-text">
        I‚Äôm a 28-year-old Saudi üá∏üá¶ researcher focused on multimodal AI ü§ñ and vision-language models üëÅÔ∏è, with a
        strong emphasis on motion perception üèÉüèΩ‚Äç‚ôÇÔ∏è, trajectory generation, and safety-aligned interpretable planning
        for autonomous systems üöó.

        <!-- I‚Äôm a 28-year-old Saudi üá∏üá¶ researcher passionate about multimodal AI, vision-language models, and motion perception & generation. Recently I‚Äôve built instruction-tuned motion predictors (iMotion-LLM), vision-language driving systems (iMotion-VLM), and large-scale benchmarks (ReefNet) while stress-testing autonomy stacks in simulation and the real world. I design vision-centric autonomy stacks that stay reliable when conditions get messy‚Äîharsh weather, rare edge cases, and the long tail of real-world driving. My work blends scalable data pipelines, foundation models for perception, and planning architectures that balance safety with smooth, human-like driving. I‚Äôm seeking a research internship across AI (LLMs, VLMs, MLLMs, motion perception/generation), with a strong track record in autonomy but open to broader AI roles. -->
      </p>
      <!-- <div class="grid three">
        <div class="card">
          <h3>Perception</h3>
          <p>Multimodal fusion (camera, LiDAR, radar) with self-supervision to keep detections crisp in night, fog, and glare.</p>
        </div>
        <div class="card">
          <h3>Reasoning</h3>
          <p>LLMs/VLMs/MLLMs with instruction tuning (SFT/DPO/GRPO) and VLAs to explain, predict, and act in dynamic scenes.</p>
        </div>
        <div class="card">
          <h3>Planning</h3>
          <p>Decision layers that reason about uncertainty, social cues, and comfort‚Äîso vehicles drive assertively yet safely.</p>
        </div>
      </div> -->
    </section>

    <section class="section" id="research-focus">
      <div class="section__header">
        <div>
          <p class="eyebrow">Research focus</p>
          <h2>Multimodal AI with an autonomy edge</h2>
        </div>
      </div>

      <div class="grid three">
        <div class="card">
          <h3>Interests</h3>
          <p>LLMs, VLMs, MLLMs, and VLAs for perception and generation; motion forecasting; spatio-temporal models;
            reasoning-centric fine-tuning; large-scale dataset processing and evaluation.</p>
        </div>

        <div class="card">
          <h3>AI Tooling & Skills</h3>
          <p>Python, PyTorch, OpenCV; multi-GPU training and Slurm; AD tooling (open/closed-loop, BEV/trajectory
            representations); data curation pipelines; graph/spatio-temporal models; MATLAB and basic C++.</p>
        </div>

        <div class="card">
          <h3>Engineering Foundations</h3>
          <p>Before diving into AI, I spent five years studying electrical and computer engineering, building strong
            foundations in digital signal processing, digital image processing, seismic signal processing, wireless
            communication systems, and ultrasonic-based indoor localization.</p>
        </div>
      </div>
      </div>
    </section>

    <section class="section" id="publications">
      <div class="section__header">
        <div>
          <p class="eyebrow">Publications</p>
          <h2>Selected papers & patents</h2>
        </div>
      </div>
      <div class="list">
        <div class="list-item">
          <div class="badge">WACV 2026</div>
          <div class="pub-flex">
            <img src="images/imotion_llm_teaset.png" alt="iMotion-LLM teaser" class="pub-teaser">
            <div>
              <h3>iMotion-LLM: Instruction-Conditioned Trajectory Generation</h3>
              <p class="meta">
                <span class="author-self">Abdulwahab Felemban</span>, Nussair Hroub, Jian Ding, Eslam Abdelrahman,
                Xiaoqian Shen, Abduallah Mohamed$, Mohamed Elhoseiny
              </p>
              <p class="meta">Instruction-conditioned trajectory generation with feasibility- and safety-aligned motion
                prediction.</p>
              <div class="pub-links">
                <a href="https://arxiv.org/pdf/2406.06211" target="_blank" rel="noreferrer">
                  <img src="images/arxiv.png" alt="arXiv" class="icon-img">
                </a>
                <a href="https://vision-cair.github.io/iMotion-LLM/" target="_blank" rel="noreferrer">
                  <img src="images/web.png" alt="Project page" class="icon-img">
                </a>
                <a href="https://github.com/Vision-CAIR/iMotion-LLM" target="_blank" rel="noreferrer">
                  <img src="images/github.png" alt="GitHub repo" class="icon-img">
                </a>
              </div>
            </div>
          </div>
        </div>
        <div class="list-item">
          <div class="badge">ArXiv 2025</div>
          <div class="pub-flex">
            <img src="images/reefnet_teaser.png" alt="ReefNet teaser" class="pub-teaser">
            <div>
              <h3>ReefNet: A Large scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification
              </h3>
              <p class="meta">
                Yahia Battach*,
                <span class="author-self">Abdulwahab Felemban*</span>,
                Faizan Farooq Khan, Yousef A. Radwan, Xiang Li, Fabio Marchese, Sara Beery, Burton H. Jones, Francesca
                Benzoni, Mohamed Elhoseiny
              </p>
              <p class="meta">Taxonomically enriched hard-coral dataset and benchmark with rigorous verification
                pipeline. * Equal contribution.</p>
              <div class="pub-links">
                <a href="https://www.arxiv.org/pdf/2510.16822" target="_blank" rel="noreferrer">
                  <img src="images/arxiv.png" alt="arXiv" class="icon-img">
                </a>
              </div>
            </div>
          </div>
        </div>
        <div class="list-item">
          <div class="badge">Patent</div>
          <div>
            <h3>Real-Time Control of Mechanical Valves Using Computer Vision and Machine Learning</h3>
            <p class="meta">US Patent App. 18/021,241 (2023) with Tareq Y. Al-Naffouri, Mohamed Masood, <span
                class="author-self">Abdulwahab Felemban</span>, Mohannad F. Aljoud, Ayman Bader, Mohammad T. Alkhodary,
              Tariq B. K. Ahmed.</p>
          </div>
        </div>
        <div class="list-item">
          <div class="badge">ArXiv 2025</div>
          <div>
            <h3>FishNet++: Analyzing the capabilities of Multimodal Large Language Models in marine biology</h3>
            <p class="meta">Faizan Farooq Khan, Yousef A. Radwan, Eslam M. Bakr, <span class="author-self">Abdulwahab
                Felemban</span>, Ali Mir, Nicholas K. Michiels, Arwa J. Temple, Andrew J. Berumen, Mohamed Elhoseiny.
            </p>
            <div class="pub-links">
              <a href="https://arxiv.org/pdf/2509.25564" target="_blank" rel="noreferrer">
                <img src="images/arxiv.png" alt="arXiv" class="icon-img">
              </a>
            </div>
          </div>
        </div>
        <div class="list-item">
          <div class="badge">ArXiv 2024</div>
          <div>
            <h3>How Well Can Vision Language Models See Image Details?</h3>
            <p class="meta">Chengzhi Gou, <span class="author-self">Abdulwahab Felemban</span>, Faizan Farooq Khan, Deyu
              Zhu, Jing Cai, Hamid Rezatofighi, Mohamed Elhoseiny.</p>
            <div class="pub-links">
              <a href="https://arxiv.org/pdf/2408.03940" target="_blank" rel="noreferrer">
                <img src="images/arxiv.png" alt="arXiv" class="icon-img">
              </a>
            </div>
          </div>
        </div>
        <div class="list-item">
          <div class="badge">EUSIPCO 2020</div>
          <div>
            <h3>Robust 2D Indoor Positioning Algorithm in the Presence of Non-Line-of-Sight Signals</h3>
            <p class="meta">Mohammed H. AlSharif, Mutaz Ahmed, <span class="author-self">Abdulwahab Felemban</span>,
              Abdulrahman Zayat, Abdulhadi Muqaibel, Mohamed Masood, Tareq Y. Al-Naffouri.</p>
            <div class="pub-links">
              <a href="https://ieeexplore.ieee.org/document/9287812" target="_blank" rel="noreferrer">
                <img src="images/web.png" alt="IEEE" class="icon-img">
              </a>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="projects">
      <div class="section__header">
        <div>
          <p class="eyebrow">Projects</p>
          <h2>Project Highlights</h2>
        </div>
      </div>
      <div class="grid two">
        <div class="card">
          <h3>iMotion-VLM</h3>
          <p>End-to-end vision-language autonomous driving with multi-step reasoning and planning on nuScenes.</p>
        </div>
        <div class="card">
          <h3>InstructNuPlan & InstructWaymo</h3>
          <p>Safety- and feasibility-aligned trajectory datasets using vectorized BEV representations and instruction
            following.</p>
        </div>
        <div class="card">
          <h3>ReefNet</h3>
          <p>925K-annotation benchmark for coral identification; ViT/BEiT/BioCLIP/CLIP/SigLIP baselines with quality
            control.</p>
        </div>
        <div class="card">
          <h3>Smart-Tap</h3>
          <p>Real-time AI water valve control with spatio-temporal GCNs; prize-winning prototype for anticipatory
            actions.</p>
        </div>
        <div class="card">
          <h3>LLM/VLM fine-tuning</h3>
          <p>Reasoning-centric SFT/DPO/GRPO pipelines for instruction-following, motion prediction, and visual detail
            grounding.</p>
        </div>
        <div class="card">
          <h3>Scenario/data engines</h3>
          <p>Large-scale data curation, simulation stress-testing, and evaluation dashboards for both autonomy and
            general vision-language tasks.</p>
        </div>
      </div>
    </section>

    <section class="section" id="education">
      <div class="section__header">
        <div>
          <p class="eyebrow">Education</p>
          <h2>Where I‚Äôve been learning</h2>
        </div>
      </div>
      <div class="timeline">
        <div class="timeline__item">
          <div class="timeline__dot"></div>
          <div class="timeline__content">
            <div class="edu">
              <img src="images/kaust.png" alt="KAUST logo" class="edu-logo">
              <div>
                <h3>Ph.D. ¬∑ Electrical & Computer Engineering</h3>
                <p class="meta">KAUST ‚Äî 01/2022‚Äì06/2027 (expected)</p>
                <p>Vision-CAIR with Prof. Mohammad Elhoseiny. Focus: deep learning, motion forecasting, reasoning for
                  autonomous driving.</p>
              </div>
            </div>
          </div>
        </div>
        <div class="timeline__item">
          <div class="timeline__dot"></div>
          <div class="timeline__content">
            <div class="edu">
              <img src="images/kaust.png" alt="KAUST logo" class="edu-logo">
              <div>
                <h3>M.Sc. ¬∑ Electrical & Computer Engineering</h3>
                <p class="meta">KAUST ‚Äî 01/2020‚Äì01/2022 | GPA: 3.83/4.0</p>
                <p>Advisor: Prof. Tareq Al-Naffouri. Deep learning for human action prediction.</p>
              </div>
            </div>
          </div>
        </div>
        <div class="timeline__item">
          <div class="timeline__dot"></div>
          <div class="timeline__content">
            <div class="edu">
              <img src="images/kfupm.png" alt="KFUPM logo" class="edu-logo">
              <div>
                <h3>B.Sc. ¬∑ Electrical Engineering</h3>
                <p class="meta">KFUPM ‚Äî 08/2014‚Äì12/2019 | GPA: 3.43/4.0</p>
                <p>Signal processing and wireless communications; AI surveillance senior project; heatstroke sensor
                  junior project.</p>
              </div>
            </div>
          </div>
        </div>
        <div class="timeline__item">
          <div class="timeline__dot"></div>
          <div class="timeline__content">
            <div class="edu">
              <img src="images/gatech.png" alt="Georgia Tech logo" class="edu-logo">
              <div>
                <h3>Exchange Semester ¬∑ Electrical Engineering</h3>
                <p class="meta">Georgia Institute of Technology ‚Äî 08/2018‚Äì12/2018</p>
                <p>Coursework in controls and signal processing; broadened international perspective.</p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="awards">
      <div class="section__header">
        <div>
          <p class="eyebrow">Awards</p>
          <h2>Recognitions & funding</h2>
        </div>
      </div>
      <div class="list">
        <div class="list-item">
          <div class="badge">Funding</div>
          <div>
            <h3>$1M research funding</h3>
            <p class="meta">NEOM OSSARI ($600k, 2024) + KAUST Translational Fund ($450k, 2022).</p>
          </div>
        </div>
        <div class="list-item">
          <div class="badge">Awards</div>
          <div>
            <h3>Project wins</h3>
            <p class="meta">Start Smart 2 (1st, $9k, EcoStream, 2024); Digital Innovation Award 2 (1st, Smart-Tap, $21k,
              2022); KFUPM Senior-Project (1st, AI Surveillance, 2019); SSI Poster (2nd, Indoor localization, 2019).</p>
          </div>
        </div>
        <div class="list-item">
          <div class="badge">Academic</div>
          <div>
            <h3>CEMSE Dean‚Äôs List</h3>
            <p class="meta">Awarded to top 20% students, 2021‚Äì2022.</p>
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="news">
      <div class="section__header">
        <div>
          <p class="eyebrow">News</p>
          <h2>Recent News</h2>
        </div>
      </div>
      <div class="list">
        <div class="list-item">
          <div class="badge">Nov 2025</div>
          <div>
            <h3>Paper Accepted at WACV 2024 to the Main Track</h3>
          </div>
        </div>
        <div class="list-item">
          <div class="badge">Jan 2025</div>
          <div>
            <h3>Taught 1-week Introduction to AI at the KAUST Academy</h3>
            <p class="meta">
              <a href="https://www.linkedin.com/posts/abdulwahab-felemban-b61469144_activity-7284541842405154817-DxYU?utm_source=share&utm_medium=member_desktop&rcm=ACoAACLzmTkBn5ssXXutW1FXKUKYz8_10A1TvDQ"
                target="_blank" rel="noopener noreferrer" class="inline-link">View Post</a>
            </p>
          </div>
        </div>
        <div class="list-item">
          <div class="badge">Jan 2024</div>
          <div>
            <h3>ReefNet project secured $600k funding from NEOM OSSARI</h3>
          </div>
        </div>
        <div class="list-item">
          <div class="badge">Mar 2023</div>
          <div>
            <h3>Ecostream awarded 1st place at Start Smart competition</h3>
            <p class="meta">
              <a href="https://www.ecostream-sa.com/2024/11/28/195/" target="_blank" rel="noopener noreferrer"
                class="inline-link">View Announcement</a>
            </p>
          </div>
        </div>
        <div class="list-item">
          <div class="badge">Aug 2022</div>
          <div>
            <h3>Awarded CEMSE Dean‚Äôs Award</h3>
          </div>
        </div>
        <div class="list-item">
          <div class="badge">Jan 2022</div>
          <div>
            <h3>Smart-Tap project secured $450k Research Translation Funding</h3>
          </div>
        </div>
        <div class="list-item">
          <div class="badge">May 2021</div>
          <div>
            <h3>Smart-Tap won 1st Place in the Digital Innovation Award by MCIT</h3>
            <p class="meta">
              <a href="https://x.com/kaust_newsar/status/1397932407182041095?s=21" target="_blank"
                rel="noopener noreferrer" class="inline-link">View Announcement</a>
            </p>
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="service">
      <div class="section__header">
        <div>
          <p class="eyebrow">Service</p>
          <h2>Community contributions</h2>
        </div>
      </div>
      <div class="grid two">
        <div class="card">
          <h3>Reviewer</h3>
          <p>WACV 2026; ICCV 2025 Workshop Proceedings & CV4E; ACML 2024; SIGGRAPH Asia 2024.</p>
        </div>
        <div class="card">
          <h3>Organizer & Instructor</h3>
          <p>CVPR 2024 Workshop C3DV co-organizer; KAUST Academy 1-week AI instructor; KFUPM IEEE Publicity Chair; SSI
            indoor localization project.</p>
        </div>
      </div>
    </section>
  </main>
</body>

</html>